<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Solvers · NeuralVerification.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">NeuralVerification.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">NeuralVerification.jl</a></li><li><a class="tocitem" href="../problem/">Problem Definitions</a></li><li class="is-active"><a class="tocitem" href>Solvers</a><ul class="internal"><li><a class="tocitem" href="#Reachability-Methods"><span>Reachability Methods</span></a></li><li><a class="tocitem" href="#Primal-Optimization-Methods"><span>Primal Optimization Methods</span></a></li><li><a class="tocitem" href="#Dual-Optimization-Methods"><span>Dual Optimization Methods</span></a></li><li><a class="tocitem" href="#Search-and-Reachability-Methods"><span>Search and Reachability Methods</span></a></li><li><a class="tocitem" href="#Search-and-Optimization-Methods"><span>Search and Optimization Methods</span></a></li></ul></li><li><a class="tocitem" href="../functions/">Helper Functions</a></li><li><a class="tocitem" href="../existing_implementations/">Existing Implementations</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sisl/NeuralVerification.jl/blob/master/docs/src/solvers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Solvers"><a class="docs-heading-anchor" href="#Solvers">Solvers</a><a id="Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Solvers" title="Permalink"></a></h1><p>The three basic verification methods are &quot;reachability&quot;, &quot;optimization&quot;, and &quot;search&quot;. These are further divided into the five categories listed below. Note that all of the optimization methods use the <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP.jl</a> library.</p><ul><li><a href="#Solvers">Solvers</a></li><ul><li><a href="#Reachability-Methods">Reachability Methods</a></li><ul><li><a href="#ExactReach">ExactReach</a></li><li><a href="#Ai2">Ai2</a></li><li><a href="#MaxSens">MaxSens</a></li></ul><li><a href="#Primal-Optimization-Methods">Primal Optimization Methods</a></li><ul><li><a href="#NSVerify">NSVerify</a></li><li><a href="#MIPVerify">MIPVerify</a></li><li><a href="#ILP">ILP</a></li></ul><li><a href="#Dual-Optimization-Methods">Dual Optimization Methods</a></li><ul><li><a href="#Duality">Duality</a></li><li><a href="#ConvDual">ConvDual</a></li><li><a href="#Certify">Certify</a></li></ul><li><a href="#Search-and-Reachability-Methods">Search and Reachability Methods</a></li><ul><li><a href="#ReluVal">ReluVal</a></li><li><a href="#FastLin">FastLin</a></li><li><a href="#FastLip">FastLip</a></li><li><a href="#DLV">DLV</a></li></ul><li><a href="#Search-and-Optimization-Methods">Search and Optimization Methods</a></li><ul><li><a href="#Sherlock">Sherlock</a></li><li><a href="#BaB">BaB</a></li><li><a href="#Planet">Planet</a></li><li><a href="#Reluplex">Reluplex</a></li></ul></ul></ul><h2 id="Reachability-Methods"><a class="docs-heading-anchor" href="#Reachability-Methods">Reachability Methods</a><a id="Reachability-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Reachability-Methods" title="Permalink"></a></h2><p>These methods perform exact or approximate reachability analysis to determine the output set corresponding to a given input set. In addition, <code>MaxSens</code>, which computes lower and upper bounds for each layer, is called within other solver types in the form of <a href="@ref"><code>get_bounds</code></a>.</p><h3 id="ExactReach"><a class="docs-heading-anchor" href="#ExactReach">ExactReach</a><a id="ExactReach-1"></a><a class="docs-heading-anchor-permalink" href="#ExactReach" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.ExactReach" href="#NeuralVerification.ExactReach"><code>NeuralVerification.ExactReach</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ExactReach</code></pre><p>ExactReach performs exact reachability analysis to compute the output reachable set for a network.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: HPolytope</li><li>Output: AbstractPolytope</li></ol><p><strong>Return</strong></p><p><code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Exact reachability analysis.</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1712.08163">W. Xiang, H.-D. Tran, and T. T. Johnson, &quot;Reachable Set Computation and Safety Verification for Neural Networks with ReLU Activations,&quot; <em>ArXiv Preprint ArXiv:1712.08163</em>, 2017.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/9e222fe6edf83a5808d5081b8338c7b66f7560fe/src/reachability/exactReach.jl#L1-L24">source</a></section></article><h3 id="Ai2"><a class="docs-heading-anchor" href="#Ai2">Ai2</a><a id="Ai2-1"></a><a class="docs-heading-anchor-permalink" href="#Ai2" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.Ai2" href="#NeuralVerification.Ai2"><code>NeuralVerification.Ai2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Ai2{T}</code></pre><p><code>Ai2</code> performs over-approximated reachability analysis to compute the over-approximated output reachable set for a network. <code>T</code> can be <code>Hyperrectangle</code>, <code>Zonotope</code>, or <code>HPolytope</code>, and determines the amount of over-approximation (and hence also performance tradeoff). The original implementation (from [1]) uses Zonotopes, so we consider this the &quot;benchmark&quot; case. The <code>HPolytope</code> case is more precise, but slower, and the opposite is true of the <code>Hyperrectangle</code> case.</p><p>Note that initializing <code>Ai2()</code> defaults to <code>Ai2{Zonotope}</code>. The following aliases also exist for convenience:</p><pre><code class="language-julia">const Ai2h = Ai2{HPolytope}
const Ai2z = Ai2{Zonotope}
const Box = Ai2{Hyperrectangle}</code></pre><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation (more activations to be supported in the future)</li><li>Input: AbstractPolytope</li><li>Output: AbstractPolytope</li></ol><p><strong>Return</strong></p><p><code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Reachability analysis using split and join.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p>[1] T. Gehr, M. Mirman, D. Drashsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev, &quot;Ai2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation,&quot; in <em>2018 IEEE Symposium on Security and Privacy (SP)</em>, 2018.</p><p><strong>Note</strong></p><p>Efficient over-approximation of intersections and unions involving zonotopes relies on Theorem 3.1 of</p><p>[2] Singh, G., Gehr, T., Mirman, M., Püschel, M., &amp; Vechev, M. (2018). Fast and effective robustness certification. In Advances in Neural Information Processing Systems (pp. 10802-10813).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/9e222fe6edf83a5808d5081b8338c7b66f7560fe/src/reachability/ai2.jl#L1-L45">source</a></section></article><h3 id="MaxSens"><a class="docs-heading-anchor" href="#MaxSens">MaxSens</a><a id="MaxSens-1"></a><a class="docs-heading-anchor-permalink" href="#MaxSens" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.MaxSens" href="#NeuralVerification.MaxSens"><code>NeuralVerification.MaxSens</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MaxSens(resolution::Float64, tight::Bool)</code></pre><p>MaxSens performs over-approximated reachability analysis to compute the over-approximated output reachable set for a network.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, any activation that is monotone</li><li>Input: <code>Hyperrectangle</code> or <code>HPolytope</code></li><li>Output: <code>AbstractPolytope</code></li></ol><p><strong>Return</strong></p><p><code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>First partition the input space into small grid cells according to <code>resolution</code>. Then use interval arithmetic to compute the reachable set for each cell. Two versions of interval arithmetic is implemented with indicator <code>tight</code>. Default <code>resolution</code> is <code>1.0</code>. Default <code>tight = false</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1708.03322">W. Xiang, H.-D. Tran, and T. T. Johnson, &quot;Output Reachable Set Estimation and Verification for Multi-Layer Neural Networks,&quot; <em>ArXiv Preprint ArXiv:1708.03322</em>, 2017.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L26">source</a></section></article><h2 id="Primal-Optimization-Methods"><a class="docs-heading-anchor" href="#Primal-Optimization-Methods">Primal Optimization Methods</a><a id="Primal-Optimization-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Primal-Optimization-Methods" title="Permalink"></a></h2><h4 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h4><pre><code class="language-">using NeuralVerification # hide
nnet = read_nnet(&quot;../../examples/networks/small_nnet.nnet&quot;)
input  = Hyperrectangle([0.0], [.5])
output = HPolytope(ones(1,1), [102.5])

problem = Problem(nnet, input, output)
# set the JuMP solver with `optimizer` keyword or use default:
solver = MIPVerify(optimizer = GLPKSolverMIP())

solve(solver,  problem)</code></pre><h3 id="NSVerify"><a class="docs-heading-anchor" href="#NSVerify">NSVerify</a><a id="NSVerify-1"></a><a class="docs-heading-anchor-permalink" href="#NSVerify" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.NSVerify" href="#NeuralVerification.NSVerify"><code>NeuralVerification.NSVerify</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NSVerify(optimizer, m::Float64)</code></pre><p>NSVerify finds counter examples using mixed integer linear programming.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle or hpolytope</li><li>Output: PolytopeComplement</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code></p><p><strong>Method</strong></p><p>MILP encoding (using <code>m</code>). No presolve. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>. Default <code>m</code> is <code>1000.0</code> (should be large enough to avoid approximation error).</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1706.07351">A. Lomuscio and L. Maganti, &quot;An Approach to Reachability Analysis for Feed-Forward Relu Neural Networks,&quot; <em>ArXiv Preprint ArXiv:1706.07351</em>, 2017.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L24">source</a></section></article><h3 id="MIPVerify"><a class="docs-heading-anchor" href="#MIPVerify">MIPVerify</a><a id="MIPVerify-1"></a><a class="docs-heading-anchor-permalink" href="#MIPVerify" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.MIPVerify" href="#NeuralVerification.MIPVerify"><code>NeuralVerification.MIPVerify</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MIPVerify(optimizer)</code></pre><p>MIPVerify computes maximum allowable disturbance using mixed integer linear programming.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: PolytopeComplement</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>MILP encoding. Use presolve to compute a tight node-wise bounds first. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>.</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p>V. Tjeng, K. Xiao, and R. Tedrake, <a href="https://arxiv.org/abs/1711.07356">&quot;Evaluating Robustness of Neural Networks with Mixed Integer Programming,&quot; <em>ArXiv Preprint ArXiv:1711.07356</em>, 2017.</a></p><p><a href="https://github.com/vtjeng/MIPVerify.jl">https://github.com/vtjeng/MIPVerify.jl</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L27">source</a></section></article><h3 id="ILP"><a class="docs-heading-anchor" href="#ILP">ILP</a><a id="ILP-1"></a><a class="docs-heading-anchor-permalink" href="#ILP" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.ILP" href="#NeuralVerification.ILP"><code>NeuralVerification.ILP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ILP(optimizer, max_iter)</code></pre><p>ILP iteratively solves a linearized primal optimization to compute maximum allowable disturbance. It iteratively adds the linear constraint to the problem.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: PolytopeComplement</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>Iteratively solve a linear encoding of the problem. It only considers the linear piece of the network that has the same activation pattern as the reference input. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>. We provide both iterative method and non-iterative method to solve the LP problem. Default <code>iterative</code> is <code>true</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1605.07262">O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, and A. Criminisi, &quot;Measuring Neural Net Robustness with Constraints,&quot; in <em>Advances in Neural Information Processing Systems</em>, 2016.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L28">source</a></section></article><h2 id="Dual-Optimization-Methods"><a class="docs-heading-anchor" href="#Dual-Optimization-Methods">Dual Optimization Methods</a><a id="Dual-Optimization-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Dual-Optimization-Methods" title="Permalink"></a></h2><h3 id="Duality"><a class="docs-heading-anchor" href="#Duality">Duality</a><a id="Duality-1"></a><a class="docs-heading-anchor-permalink" href="#Duality" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.Duality" href="#NeuralVerification.Duality"><code>NeuralVerification.Duality</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Duality(optimizer)</code></pre><p>Duality uses Lagrangian relaxation to compute over-approximated bounds for a network</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, any activation function that is monotone</li><li>Input: hyperrectangle</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Lagrangian relaxation. Default <code>optimizer</code> is <code>GLPKSolverMIP()</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1803.06567">K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, &quot;A Dual Approach to Scalable Verification of Deep Networks,&quot; <em>ArXiv Preprint ArXiv:1803.06567</em>, 2018.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L24">source</a></section></article><h3 id="ConvDual"><a class="docs-heading-anchor" href="#ConvDual">ConvDual</a><a id="ConvDual-1"></a><a class="docs-heading-anchor-permalink" href="#ConvDual" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.ConvDual" href="#NeuralVerification.ConvDual"><code>NeuralVerification.ConvDual</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ConvDual</code></pre><p>ConvDual uses convex relaxation to compute over-approximated bounds for a network</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Convex relaxation with duality.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1711.00851">E. Wong and J. Z. Kolter, &quot;Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope,&quot; <em>ArXiv Preprint ArXiv:1711.00851</em>, 2017.</a></p><p><a href="https://github.com/locuslab/convex_adversarial">https://github.com/locuslab/convex_adversarial</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sisl/NeuralVerification.jl/blob/9e222fe6edf83a5808d5081b8338c7b66f7560fe/src/optimization/convDual.jl#L1-L25">source</a></section></article><h3 id="Certify"><a class="docs-heading-anchor" href="#Certify">Certify</a><a id="Certify-1"></a><a class="docs-heading-anchor-permalink" href="#Certify" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.Certify" href="#NeuralVerification.Certify"><code>NeuralVerification.Certify</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Certify(optimizer)</code></pre><p>Certify uses semidefinite programming to compute over-approximated certificates for a neural network with only one hidden layer.</p><p><strong>Problem requirement</strong></p><ol><li>Network: one hidden layer, any activation that is differentiable almost everywhere whose derivative is bound by 0 and 1</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Semindefinite programming. Default <code>optimizer</code> is <code>SCSSolver()</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1801.09344">A. Raghunathan, J. Steinhardt, and P. Liang, &quot;Certified Defenses against Adversarial Examples,&quot; <em>ArXiv Preprint ArXiv:1801.09344</em>, 2018.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L24">source</a></section></article><h2 id="Search-and-Reachability-Methods"><a class="docs-heading-anchor" href="#Search-and-Reachability-Methods">Search and Reachability Methods</a><a id="Search-and-Reachability-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Search-and-Reachability-Methods" title="Permalink"></a></h2><h3 id="ReluVal"><a class="docs-heading-anchor" href="#ReluVal">ReluVal</a><a id="ReluVal-1"></a><a class="docs-heading-anchor-permalink" href="#ReluVal" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.ReluVal" href="#NeuralVerification.ReluVal"><code>NeuralVerification.ReluVal</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ReluVal(max_iter::Int64, tree_search::Symbol)</code></pre><p>ReluVal combines symbolic reachability analysis with iterative interval refinement to minimize over-approximation of the reachable set.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: AbstractPolytope</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code> or <code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Symbolic reachability analysis and iterative interval refinement (search).</p><ul><li><code>max_iter</code> default <code>10</code>.</li><li><code>tree_search</code> default <code>:DFS</code> - depth first search.</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1804.10829">S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, &quot;Formal Security Analysis of Neural Networks Using Symbolic Intervals,&quot; <em>CoRR</em>, vol. abs/1804.10829, 2018. arXiv: 1804.10829.</a></p><p><a href="https://github.com/tcwangshiqi-columbia/ReluVal">https://github.com/tcwangshiqi-columbia/ReluVal</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L27">source</a></section></article><h3 id="FastLin"><a class="docs-heading-anchor" href="#FastLin">FastLin</a><a id="FastLin-1"></a><a class="docs-heading-anchor-permalink" href="#FastLin" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.FastLin" href="#NeuralVerification.FastLin"><code>NeuralVerification.FastLin</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FastLin(maxIter::Int64, ϵ0::Float64, accuracy::Float64)</code></pre><p>FastLin combines reachability analysis with binary search to find maximum allowable disturbance.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hypercube</li><li>Output: AbstractPolytope</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>Reachability analysis by network approximation and binary search.</p><ul><li><code>max_iter</code> is the maximum iteration in search, default <code>10</code>;</li><li><code>ϵ0</code> is the initial search radius, default <code>100.0</code>;</li><li><code>accuracy</code> is the stopping criteria, default <code>0.1</code>;</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1804.09699">T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel, &quot;Towards Fast Computation of Certified Robustness for ReLU Networks,&quot; <em>ArXiv Preprint ArXiv:1804.09699</em>, 2018.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L26">source</a></section></article><h3 id="FastLip"><a class="docs-heading-anchor" href="#FastLip">FastLip</a><a id="FastLip-1"></a><a class="docs-heading-anchor-permalink" href="#FastLip" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.FastLip" href="#NeuralVerification.FastLip"><code>NeuralVerification.FastLip</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FastLip(maxIter::Int64, ϵ0::Float64, accuracy::Float64)</code></pre><p>FastLip adds Lipschitz estimation on top of FastLin.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hypercube</li><li>Output: halfspace</li></ol><p><strong>Return</strong></p><p><code>AdversarialResult</code></p><p><strong>Method</strong></p><p>Lipschitz estimation + FastLin. All arguments are for FastLin.</p><ul><li><code>max_iter</code> is the maximum iteration in search, default <code>10</code>;</li><li><code>ϵ0</code> is the initial search radius, default <code>100.0</code>;</li><li><code>accuracy</code> is the stopping criteria, default <code>0.1</code>;</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1804.09699">T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel, &quot;Towards Fast Computation of Certified Robustness for ReLU Networks,&quot; <em>ArXiv Preprint ArXiv:1804.09699</em>, 2018.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L26">source</a></section></article><h3 id="DLV"><a class="docs-heading-anchor" href="#DLV">DLV</a><a id="DLV-1"></a><a class="docs-heading-anchor-permalink" href="#DLV" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.DLV" href="#NeuralVerification.DLV"><code>NeuralVerification.DLV</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DLV(ϵ::Float64)</code></pre><p>DLV searches layer by layer for counter examples in hidden layers.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, any activation (currently only support ReLU)</li><li>Input: Hyperrectangle</li><li>Output: AbstractPolytope</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code></p><p><strong>Method</strong></p><p>The following operations are performed layer by layer. for layer i</p><ol><li>determine a reachable set from the reachable set in layer i-1</li><li>determine a search tree in the reachable set by refining the search tree in layer i-1</li><li>Verify<ul><li>True -&gt; continue to layer i+1</li><li>False -&gt; counter example</li></ul></li></ol><p>The argument <code>ϵ</code> is the resolution of the initial search tree. Default <code>1.0</code>.</p><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1610.06940">X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, &quot;Safety Verification of Deep Neural Networks,&quot; in <em>International Conference on Computer Aided Verification</em>, 2017.</a></p><p><a href="https://github.com/VeriDeep/DLV">https://github.com/VeriDeep/DLV</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L32">source</a></section></article><h2 id="Search-and-Optimization-Methods"><a class="docs-heading-anchor" href="#Search-and-Optimization-Methods">Search and Optimization Methods</a><a id="Search-and-Optimization-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Search-and-Optimization-Methods" title="Permalink"></a></h2><h3 id="Sherlock"><a class="docs-heading-anchor" href="#Sherlock">Sherlock</a><a id="Sherlock-1"></a><a class="docs-heading-anchor-permalink" href="#Sherlock" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.Sherlock" href="#NeuralVerification.Sherlock"><code>NeuralVerification.Sherlock</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Sherlock(optimizer, ϵ::Float64)</code></pre><p>Sherlock combines local and global search to estimate the range of the output node.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation, single output</li><li>Input: hpolytope and hyperrectangle</li><li>Output: hyperrectangle (1d interval)</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code> or <code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Local search: solve a linear program to find local optima on a line segment of the piece-wise linear network. Global search: solve a feasibilty problem using MILP encoding (default calling NSVerify).</p><ul><li><code>optimizer</code> default <code>GLPKSolverMIP()</code></li><li><code>ϵ</code> is the margin for global search, default <code>0.1</code>.</li></ul><p><strong>Property</strong></p><p>Sound but not complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1709.09130">S. Dutta, S. Jha, S. Sanakaranarayanan, and A. Tiwari, &quot;Output Range Analysis for Deep Neural Networks,&quot; <em>ArXiv Preprint ArXiv:1709.09130</em>, 2017.</a></p><p><a href="https://github.com/souradeep-111/sherlock">https://github.com/souradeep-111/sherlock</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L28">source</a></section></article><h3 id="BaB"><a class="docs-heading-anchor" href="#BaB">BaB</a><a id="BaB-1"></a><a class="docs-heading-anchor-permalink" href="#BaB" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.BaB" href="#NeuralVerification.BaB"><code>NeuralVerification.BaB</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BaB(optimizer, ϵ::Float64)</code></pre><p>BaB uses branch and bound to estimate the range of the output node.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation, single output</li><li>Input: hyperrectangle</li><li>Output: hyperrectangle (1d interval)</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code> or <code>ReachabilityResult</code></p><p><strong>Method</strong></p><p>Branch and bound. For branch, it uses iterative interval refinement. For bound, it computes concrete bounds by sampling, approximated bound by optimization.</p><ul><li><code>optimizer</code> default <code>GLPKSolverMIP()</code></li><li><code>ϵ</code> is the desired accurancy for termination, default <code>0.1</code>.</li></ul><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1711.00455">R. Bunel, I. Turkaslan, P. H. Torr, P. Kohli, and M. P. Kumar, &quot;A Unified View of Piecewise Linear Neural Network Verification,&quot; <em>ArXiv Preprint ArXiv:1711.00455</em>, 2017.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L27">source</a></section></article><h3 id="Planet"><a class="docs-heading-anchor" href="#Planet">Planet</a><a id="Planet-1"></a><a class="docs-heading-anchor-permalink" href="#Planet" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.Planet" href="#NeuralVerification.Planet"><code>NeuralVerification.Planet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Planet(optimizer, eager::Bool)</code></pre><p>Planet integrates a SAT solver (<code>PicoSAT.jl</code>) to find an activation pattern that maps a feasible input to an infeasible output.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle or hpolytope</li><li>Output: PolytopeComplement</li></ol><p><strong>Return</strong></p><p><code>BasicResult</code></p><p><strong>Method</strong></p><p>Binary search of activations (0/1) and pruning by optimization. Our implementation is non eager.</p><ul><li><code>optimizer</code> default <code>GLPKSolverMIP()</code>;</li><li><code>eager</code> default <code>false</code>;</li></ul><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1705.01320">R. Ehlers, &quot;Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks,&quot; in <em>International Symposium on Automated Technology for Verification and Analysis</em>, 2017.</a></p><p><a href="https://github.com/progirep/planet">https://github.com/progirep/planet</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L26">source</a></section></article><h3 id="Reluplex"><a class="docs-heading-anchor" href="#Reluplex">Reluplex</a><a id="Reluplex-1"></a><a class="docs-heading-anchor-permalink" href="#Reluplex" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralVerification.Reluplex" href="#NeuralVerification.Reluplex"><code>NeuralVerification.Reluplex</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Reluplex(optimizer, eager::Bool)</code></pre><p>Reluplex uses binary tree search to find an activation pattern that maps a feasible input to an infeasible output.</p><p><strong>Problem requirement</strong></p><ol><li>Network: any depth, ReLU activation</li><li>Input: hyperrectangle</li><li>Output: PolytopeComplement</li></ol><p><strong>Return</strong></p><p><code>CounterExampleResult</code></p><p><strong>Method</strong></p><p>Binary search of activations (0/1) and pruning by optimization.</p><p><strong>Property</strong></p><p>Sound and complete.</p><p><strong>Reference</strong></p><p><a href="https://arxiv.org/abs/1702.01135">G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, &quot;Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,&quot; in <em>International Conference on Computer Aided Verification</em>, 2017.</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/44fa15b1502a45eac76c9017af94332d4557b251/base/#L0-L23">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../problem/">« Problem Definitions</a><a class="docs-footer-nextpage" href="../functions/">Helper Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 5 August 2020 10:50">Wednesday 5 August 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
